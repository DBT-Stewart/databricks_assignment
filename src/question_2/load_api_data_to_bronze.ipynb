{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3850aa3a-09c2-47d1-8295-8d66e43fffeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import current_date, lit\n",
    "\n",
    "# Set Delta destination path\n",
    "volume_path = \"/Volumes/my_catalog/my_schema/my_volume\"\n",
    "delta_path = f\"{volume_path}/site_info/person_info\"\n",
    "\n",
    "# Custom schema\n",
    "user_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"avatar\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Step 1: Fetch all paginated data\n",
    "page = 2\n",
    "all_users = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://reqres.in/api/users?page={page}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        break\n",
    "    \n",
    "    data = response.json()\n",
    "    users = data.get(\"data\", [])\n",
    "    \n",
    "    if not users:\n",
    "        break\n",
    "    \n",
    "    all_users.extend(users)\n",
    "    page += 1\n",
    "\n",
    "# Step 2: Create DataFrame directly (without sparkContext)\n",
    "df_final = spark.createDataFrame(all_users, schema=user_schema)\n",
    "\n",
    "# Step 3: Add site_address and load_date\n",
    "df_final = df_final.withColumn(\"site_address\", lit(\"reqres.in\")) \\\n",
    "                   .withColumn(\"load_date\", current_date())\n",
    "\n",
    "# Step 4: Write as Delta\n",
    "df_final.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Optional preview\n",
    "df_final.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2531a60-a8c6-444c-9b62-6f1de9b77a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import current_date, lit\n",
    "\n",
    "# Create Spark session (optional in notebooks)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Volume path for Delta table\n",
    "volume_path = \"/Volumes/my_catalog/my_schema/my_volume\"\n",
    "delta_path = f\"{volume_path}/site_info/person_info\"\n",
    "\n",
    "# Define custom schema for the \"data\" part of API\n",
    "user_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"avatar\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Step 1: Collect users from paginated API\n",
    "page = 2\n",
    "all_users = []\n",
    "\n",
    "while True:\n",
    "    response = requests.get(f\"https://reqres.in/api/users?page={page}\")\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        break\n",
    "    \n",
    "    data = response.json()\n",
    "    user_data = data.get(\"data\", [])\n",
    "    \n",
    "    if not user_data:\n",
    "        break\n",
    "    \n",
    "    all_users.extend(user_data)\n",
    "    page += 1\n",
    "\n",
    "# Step 2: Convert list of dicts to DataFrame\n",
    "df_users = spark.createDataFrame(all_users, schema=user_schema)\n",
    "\n",
    "# Step 3: Add site_address and load_date\n",
    "df_final = df_users.withColumn(\"site_address\", lit(\"reqres.in\")) \\\n",
    "                   .withColumn(\"load_date\", current_date())\n",
    "\n",
    "# Step 4: Write to Delta format\n",
    "df_final.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Optional: Preview\n",
    "df_final.display(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load_api_data_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
