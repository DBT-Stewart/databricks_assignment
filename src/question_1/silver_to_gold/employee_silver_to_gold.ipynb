{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d3a5280-2b66-4ea5-a067-01e39b8f072f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../source_to_bronze/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a8bd0b-524e-4908-ac87-dacd18bd571f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %run ../source_to_bronze/utils\n",
    "\n",
    "from pyspark.sql.functions import col, count, avg, sum as sum_, desc\n",
    "\n",
    "# Define paths\n",
    "volume_path = \"/Volumes/my_catalog/my_schema/my_volume\"\n",
    "silver_path = f\"{volume_path}/silver/employee_info/dim_employee\"\n",
    "bronze_path = f\"{volume_path}/bronze\"\n",
    "gold_path = f\"{volume_path}/gold/employee\"\n",
    "\n",
    "# Step 1: Read silver data (employee delta table)\n",
    "df = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "# Step 2: Read department and country CSVs from bronze\n",
    "department_df = spark.read.option(\"header\", True).csv(f\"{bronze_path}/department\")\n",
    "country_df = spark.read.option(\"header\", True).csv(f\"{bronze_path}/country\")\n",
    "\n",
    "# Step 3: Convert column names to snake_case\n",
    "department_df = rename_columns_to_snake_case(department_df)\n",
    "country_df = rename_columns_to_snake_case(country_df)\n",
    "\n",
    "# Step 4: Perform joins using corrected snake_case column names\n",
    "joined_df = df.join(department_df, df[\"department\"] == department_df[\"department_i_d\"], \"left\") \\\n",
    "              .join(country_df, df[\"country\"] == country_df[\"country_code\"], \"left\")\n",
    "\n",
    "# Step 5: Add at_load_date column\n",
    "joined_df = add_at_load_date(joined_df)\n",
    "\n",
    "# GOLD Layer: Save various business metrics\n",
    "\n",
    "# 1.Total Salary per Department (Descending)\n",
    "salary_df = joined_df.groupBy(\"department_name\") \\\n",
    "    .agg(sum_(\"salary\").alias(\"total_salary\")) \\\n",
    "    .orderBy(desc(\"total_salary\"))\n",
    "\n",
    "salary_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_path}/fact_employee_salary\")\n",
    "\n",
    "# 2.Employee count per department per country\n",
    "emp_count_df = joined_df.groupBy(\"department_name\", \"country_name\") \\\n",
    "    .agg(count(\"*\").alias(\"employee_count\"))\n",
    "\n",
    "emp_count_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_path}/fact_employee_count\")\n",
    "\n",
    "# 3.Distinct departments with country names\n",
    "dept_country_df = joined_df.select(\"department_name\", \"country_name\").distinct()\n",
    "\n",
    "dept_country_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_path}/fact_dept_country\")\n",
    "\n",
    "# 4.Average age per department\n",
    "avg_age_df = joined_df.groupBy(\"department_name\") \\\n",
    "    .agg(avg(\"age\").alias(\"average_age\"))\n",
    "\n",
    "avg_age_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_path}/fact_avg_age\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "853e6551-6331-4856-9841-b73a6eb56b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "volume_path = \"/Volumes/my_catalog/my_schema/my_volume/gold/employee\"\n",
    "\n",
    "# For example:\n",
    "df1 = spark.read.format(\"delta\").load(f\"{volume_path}/fact_employee_salary\")\n",
    "df1.display()\n",
    "\n",
    "df2 = spark.read.format(\"delta\").load(f\"{volume_path}/fact_employee_count\")\n",
    "df2.display()\n",
    "\n",
    "df3 = spark.read.format(\"delta\").load(f\"{volume_path}/fact_avg_age\")\n",
    "df3.display()\n",
    "\n",
    "df4 = spark.read.format(\"delta\").load(f\"{volume_path}/fact_dept_country\")\n",
    "df4.display()\n",
    "\n",
    "joined_df.display()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "employee_silver_to_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
